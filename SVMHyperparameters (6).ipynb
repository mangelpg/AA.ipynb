{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45cWJKARCT2Y",
        "outputId": "1dd07234-aa48-4756-80e4-27228e3464c8"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn==0.24.2\n",
        "# !pip install scikit-optimize\n",
        "# !pip install --user git+https://github.com/scikit-optimize/scikit-optimize.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkKc_paMZ-Oh",
        "outputId": "4b5b42ef-e367-4164-f884-298752c136a5"
      },
      "outputs": [],
      "source": [
        "# Checking that everything is correct with skopt (0.9.dev0) and sklearn \n",
        "from skopt import __version__\n",
        "print(__version__)\n",
        "from sklearn import __version__\n",
        "print(__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjO0ZHCkZp30"
      },
      "source": [
        "# SVM HYPER-PARAMETER TUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_5O3DQkZp31"
      },
      "source": [
        "- **C :** float, default=1.0\n",
        "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
        "    \n",
        "- **gamma :** float, default=’scale’\n",
        "Kernel coefficient for ‘rbf’\n",
        "\n",
        "- There are more hyper-parameters, but those two are the important ones: \n",
        "  - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
        "  - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tfYm7QAZp32"
      },
      "source": [
        "First, data is loaded, inputs go to X, outputs to y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJD421uMZp33"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from scipy.stats import sem\n",
        "\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQOtMG7hZp35"
      },
      "source": [
        "## COMBINING HYPER-PARAMETER TUNING AND MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ry14avEZp36"
      },
      "source": [
        "The combination of model evaluation and hyper-parameter tuning can be understood as an external loop (outer) that trains a model and tests the model, and an internal loop (inner), where the training process consists on looking for the best hyper-parameters, and then obtaining the model with those best hyper-parameters.\n",
        "\n",
        "First, we are going to use **Holdout** (train/test) for model evaluation (external loop or **outer**), and **3-fold crossvalidation** for hyper-parameter tuning (internal loop or **inner**). Hyper-parameters will be adjusted with **Gridsearch**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQQaJrViZp37"
      },
      "source": [
        "#### GRIDSEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rqhsc9OARd3"
      },
      "source": [
        "First of all, let's define our our python function for RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM7SzJLHAQhZ"
      },
      "outputs": [],
      "source": [
        "def rmse(y_test, y_test_pred):\n",
        "  \"\"\" This is my computation of Root Mean Squared Error \"\"\"\n",
        "  return np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laxLj26PZp37"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Holdout for model evaluation. 33% of available data for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-SI_kTr82RI"
      },
      "source": [
        "First, let's remember RMSE with default hyper-parameteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU6g-6PZ87DS",
        "outputId": "0e9870b0-fdfd-492c-c816-127fb098d1ab"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# This is the preprocessing pipeline: SVMs need scaling\n",
        "scaler = StandardScaler()\n",
        "svr = SVR()\n",
        "\n",
        "pipe_regr = Pipeline([\n",
        "    ('scale', scaler),\n",
        "    ('SVM', svr)])\n",
        "\n",
        "np.random.seed(42)\n",
        "pipe_regr.fit(X=X_train, y=y_train)\n",
        "print(f\"RMSE of SVR with default hyper-pars: {rmse(y_test, pipe_regr.predict(X=X_test))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRknuZjUZp39",
        "outputId": "23535874-4edb-41bd-cb1e-53acddc7f173"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Search space\n",
        "param_grid = {'SVM__C': [0.1, 1, 10, 100],\n",
        "              'SVM__gamma': [0.01, 0.1, 1]}\n",
        "\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Definition of a 2-step process that self-adjusts 2 hyperpars\n",
        "hpo_regr = GridSearchCV(pipe_regr, \n",
        "                        param_grid,\n",
        "                        scoring='neg_mean_squared_error',\n",
        "                        cv=inner, \n",
        "                        n_jobs=4, verbose=1)\n",
        "\n",
        "# Train the self-adjusting process\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)\n",
        "\n",
        "# At this point, regr contains the model with the best hyper-parameters found by gridsearch\n",
        "# and trained on the complete X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize:\n",
        "- The best hyper-parameters and their (inner!) score. \n",
        "- The outer evaluation (model evaluation) on the test partition of the model with the best hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZokHzT78udT",
        "outputId": "051cfe0c-c4f0-447e-c99e-c6575b6d6c00"
      },
      "outputs": [],
      "source": [
        "print(f\"Best params: {hpo_regr.best_params_}, best score (inner!): {np.sqrt(-hpo_regr.best_score_)}\")\n",
        "# Now, the performance of regr is computed on the test partition\n",
        "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr.predict(X=X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the best value of C is 100, which is in the border of the search space. We may consider extending the search space and see if results improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Search space\n",
        "param_grid = {'SVM__C': [0.1, 1, 10, 100, 1000, 10000],\n",
        "              'SVM__gamma': [0.01, 0.1, 1]}\n",
        "\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Definition of a 2-step process that self-adjusts 2 hyperpars\n",
        "hpo_regr = GridSearchCV(pipe_regr, \n",
        "                        param_grid,\n",
        "                        scoring='neg_mean_squared_error',\n",
        "                        cv=inner, \n",
        "                        n_jobs=4, verbose=1)\n",
        "\n",
        "# Train the self-adjusting process\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Best params: {hpo_regr.best_params_}, best score (inner!): {np.sqrt(-hpo_regr.best_score_)}\")\n",
        "# Now, the performance of regr is computed on the test partition\n",
        "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr.predict(X=X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, maybe gamma values smaller than 0.01 could be better. Let's extend the space again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Search space\n",
        "param_grid = {'SVM__C': [0.1, 1, 10, 100, 1000, 10000],\n",
        "              'SVM__gamma': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Definition of a 2-step process that self-adjusts 2 hyperpars\n",
        "hpo_regr = GridSearchCV(pipe_regr, \n",
        "                        param_grid,\n",
        "                        scoring='neg_mean_squared_error',\n",
        "                        cv=inner, \n",
        "                        n_jobs=4, verbose=1)\n",
        "\n",
        "# Train the self-adjusting process\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Best params: {hpo_regr.best_params_}, best score (inner!): {np.sqrt(-hpo_regr.best_score_)}\")\n",
        "# Now, the performance of regr is computed on the test partition\n",
        "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr.predict(X=X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk6rBqZSZp4I"
      },
      "source": [
        "#### RANDOMIZED SEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7bZU45-Zp4I"
      },
      "source": [
        "Now, let's use **Randomized Search** instead of gridsearch. Only 20 hyper-parameter value combinations will be tried (budget=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urjn49EAZp4J",
        "outputId": "e3a98fca-16f2-4a74-bcdd-811e9696c6cd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# This is the preprocessing pipeline: SVMs need scaling\n",
        "scaler = StandardScaler()\n",
        "svr = SVR()\n",
        "\n",
        "pipe_regr = Pipeline([\n",
        "    ('scale', scaler),\n",
        "    ('SVM', svr)])\n",
        "\n",
        "# Search space\n",
        "param_grid = {'SVM__C': [0.1, 1, 10, 100, 1000, 10000],\n",
        "              'SVM__gamma': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train the self-adjusting process\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)\n",
        "\n",
        "budget = 20 # out of 30 possibilities\n",
        "hpo_regr = RandomizedSearchCV(pipe_regr, \n",
        "                            param_grid,\n",
        "                            scoring='neg_mean_squared_error',\n",
        "                            cv=inner, \n",
        "                            n_jobs=4, verbose=1,\n",
        "                            n_iter=budget\n",
        "                        )\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2JJyJAn-6Ci",
        "outputId": "2841abf9-8b10-4175-a4b9-563ddb9494e5"
      },
      "outputs": [],
      "source": [
        "print(f\"Best params: {hpo_regr.best_params_}, best score (inner!): {np.sqrt(-hpo_regr.best_score_)}\")\n",
        "# Now, the performance of regr is computed on the test partition\n",
        "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr.predict(X=X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have obtained the same results, but exploring fewer possibilities than with grid-search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For **Randomized Search**, we can define the search space with statistical distributions, rather than using particular values as we did before. Below you can see how to use a loguniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.fixes import loguniform\n",
        "\n",
        "# This is the preprocessing pipeline: SVMs need scaling\n",
        "scaler = StandardScaler()\n",
        "svr = SVR()\n",
        "\n",
        "pipe_regr = Pipeline([\n",
        "    ('scale', scaler),\n",
        "    ('SVM', svr)])\n",
        "\n",
        "# Search space\n",
        "# [0.1, 1, 10, 100, 1000, 10000]\n",
        "# [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "param_grid = {'SVM__C': loguniform(1e-1, 1e4),\n",
        "              'SVM__gamma': loguniform(1e-4, 1e0)}\n",
        "\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train the self-adjusting process\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)\n",
        "\n",
        "budget = 20 \n",
        "hpo_regr = RandomizedSearchCV(pipe_regr, \n",
        "                            param_grid,\n",
        "                            scoring='neg_mean_squared_error',\n",
        "                            cv=inner, \n",
        "                            n_jobs=4, verbose=0,\n",
        "                            n_iter=budget\n",
        "                        )\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRppQBOiA9fI",
        "outputId": "8808dac2-d21e-46f5-d543-0b982843a1e3"
      },
      "outputs": [],
      "source": [
        "print(f\"Best params: {hpo_regr.best_params_}, best score (inner!): {np.sqrt(-hpo_regr.best_score_)}\")\n",
        "# Now, the performance of regr is computed on the test partition\n",
        "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr.predict(X=X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdfqhEw9Zp4W"
      },
      "source": [
        "#### OBTAINING THE FINAL MODEL (FOR DEPLOYMENT, OR FOR SENDING TO A COMPETITION, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5w47vRHZp4X"
      },
      "source": [
        "If at the end, we need a final model, we can get it by fitting hpo_regr to all the available data. Let us remember that hpo_regr does hyper.parameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kvg0RX-Zp4X",
        "outputId": "a0a7ccc8-8ed4-44b8-aec5-e2a5f3d857e5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Fitting again the randomized search HPO\n",
        "regrFinal = hpo_regr.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfsLFLTZp4Z",
        "outputId": "0883f44a-c341-4c20-ebd2-d5dee439df93"
      },
      "outputs": [],
      "source": [
        "regrFinal.best_params_, np.sqrt(-regrFinal.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XeQqmjrZp4e"
      },
      "source": [
        "#### MODEL BASED OPTIMIZATION (BAYESIAN OPTIMIZATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNwHbEIsZp4f"
      },
      "source": [
        "scikit-optimize (skopt) will be used for this: https://scikit-optimize.github.io. **Holdout** for model evaluation and **3-fold crossvalidation** for hyper-parameter tuning (with **Model Based Optimization** )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmxEk_w4cEb7",
        "outputId": "7ea71da5-cc90-423a-e95d-1036702753c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.fixes import loguniform\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# This is the preprocessing pipeline: SVMs need scaling\n",
        "scaler = StandardScaler()\n",
        "svr = SVR()\n",
        "\n",
        "pipe_regr = Pipeline([\n",
        "    ('scale', scaler),\n",
        "    ('SVM', svr)])\n",
        "\n",
        "# Search space\n",
        "# [0.1, 1, 10, 100, 1000, 10000]\n",
        "# [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "param_grid = {'SVM__C': Real(1e-1, 1e4, prior=\"loguniform\"),\n",
        "              'SVM__gamma': Real(1e-4, 1e0, prior=\"loguniform\")}\n",
        "\n",
        "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train the self-adjusting process\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)\n",
        "\n",
        "budget = 20\n",
        "hpo_regr = BayesSearchCV(pipe_regr, \n",
        "                        param_grid,\n",
        "                        scoring='neg_mean_squared_error',\n",
        "                        cv=inner, \n",
        "                        n_jobs=4, verbose=0,\n",
        "                        n_iter=budget\n",
        "                        )\n",
        "np.random.seed(42)\n",
        "hpo_regr.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIqeUTQ6Zp4k",
        "outputId": "6cdd71b3-a1f2-400d-bfe7-e8998db90e0e"
      },
      "outputs": [],
      "source": [
        "print(f\"Best params: {hpo_regr.best_params_}, best score (inner!): {np.sqrt(-hpo_regr.best_score_)}\")\n",
        "# Now, the performance of regr is computed on the test partition\n",
        "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr.predict(X=X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YSNFqSaF6Qx"
      },
      "source": [
        "We can check if the optimization has converged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "FjW8T92qFu7f",
        "outputId": "283a5368-7284-4eb9-cd71-17a5083c56fd"
      },
      "outputs": [],
      "source": [
        "_ = plot_convergence(hpo_regr.optimizer_results_[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "6yEHkN0ECMJ-",
        "outputId": "7c11b8b7-482d-4519-9587-b40e4296b57d"
      },
      "outputs": [],
      "source": [
        "_ = plot_objective(hpo_regr.optimizer_results_[0],\n",
        "                   dimensions=['max_depth', 'min_samples_split'],\n",
        "                   n_minimum_search=int(1e8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCtcKNzlCMKC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "decisionTreesHyperparameters.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
